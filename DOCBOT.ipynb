{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3465fce3-6a34-44ae-b85c-94b20c4208f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 2\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 3\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 4\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 5\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 6\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 7\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 8\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 9\n",
      "Found 10 doctor divs on this page\n",
      "Scraping page 10\n",
      "Found 10 doctor divs on this page\n",
      "Scraping complete. Data saved to doctors_in_coimbatore.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "def scrape_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    doctors = []\n",
    "\n",
    "    doctor_divs = soup.find_all('div', class_='listing-doctor-card', attrs={'data-qa-id': 'doctor_card'})\n",
    "    print(f\"Found {len(doctor_divs)} doctor divs on this page\")  # Debug line\n",
    "\n",
    "    for doctor_div in doctor_divs:\n",
    "        try:\n",
    "            name = doctor_div.find('h2', attrs={'data-qa-id': 'doctor_name'}).text.strip()\n",
    "            specialization_div = doctor_div.find('div', class_='u-grey_3-text')\n",
    "            specialization_span = specialization_div.find('span')\n",
    "            specialization = specialization_span.text.strip() if specialization_span else None\n",
    "            experience_div = specialization_div.find('div', attrs={'data-qa-id': 'doctor_experience'})\n",
    "            experience = experience_div.text.strip() if experience_div else None\n",
    "            clinic_name = doctor_div.find('span', attrs={'data-qa-id': 'doctor_clinic_name'}).text.strip()\n",
    "            fees = doctor_div.find('span', attrs={'data-qa-id': 'consultation_fee'}).text.strip()\n",
    "            locality = doctor_div.find('span', attrs={'data-qa-id': 'practice_locality'}).text.strip()\n",
    "            city = doctor_div.find('span', attrs={'data-qa-id': 'practice_city'}).text.strip()\n",
    "            address = f\"{clinic_name}, {locality}, {city}\"\n",
    "            timing_element = doctor_div.find('span', attrs={'data-qa-id': 'availability_text'})\n",
    "            timing = timing_element.text.strip() if timing_element else None\n",
    "\n",
    "            doctors.append({\n",
    "                'Name': name,\n",
    "                'Specialization': specialization,\n",
    "                'Experience': experience,\n",
    "                'Clinic Name': clinic_name,\n",
    "                'Fees': fees,\n",
    "                'Address': address,\n",
    "                'Timing': timing\n",
    "            })\n",
    "        except AttributeError as e:\n",
    "            print(f\"Error extracting data: {e}\")\n",
    "            continue\n",
    "\n",
    "    return doctors\n",
    "\n",
    "def get_next_page(soup):\n",
    "    next_button = soup.find('a', attrs={'data-qa-id': 'pagination_next'})\n",
    "    if next_button and 'href' in next_button.attrs:\n",
    "        return 'https://www.practo.com' + next_button['href']\n",
    "    return None\n",
    "\n",
    "base_url = 'https://www.practo.com/coimbatore/doctors?page=1'\n",
    "all_doctors_data = []\n",
    "page_count = 1\n",
    "\n",
    "while page_count <= 10:  # Assuming there are 10 pages\n",
    "    current_url = f\"{base_url}&page={page_count}\"\n",
    "    print(f\"Scraping page {page_count}\")\n",
    "    doctors_data = scrape_page(current_url)\n",
    "    all_doctors_data.extend(doctors_data)\n",
    "\n",
    "    page_count += 1\n",
    "    time.sleep(2)\n",
    "\n",
    "df = pd.DataFrame(all_doctors_data)\n",
    "df.to_csv('doctors_in_coimbatore.csv', index=False)\n",
    "\n",
    "print('Scraping complete. Data saved to doctors_in_coimbatore.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394163ca-5786-4547-bbb6-63455b3e1991",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
